{
  "openai_o3-mini_effort-low_single": {
    "provider": "openai_o3-mini_effort-low",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.6081982840800763,
        "recall": 0.3808955223880597,
        "f1_score": 0.4684287812041116,
        "overall_accuracy": 0.3808955223880597,
        "true_positives": 638,
        "false_positives": 411,
        "false_negatives": 1037,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.8284080076263107,
        "recall": 0.4559286463798531,
        "f1_score": 0.588155668358714,
        "overall_accuracy": 0.5188059701492538,
        "true_positives": 869,
        "true_positives_breakdown": {
          "standard": 638,
          "novel": 231
        },
        "false_positives": 180,
        "false_negatives": 1037,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "openai_o3-mini_effort-low_multi": {
    "provider": "openai_o3-mini_effort-low",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.7354260089686099,
        "recall": 0.6482213438735178,
        "f1_score": 0.6890756302521007,
        "overall_accuracy": 0.6755162241887905,
        "true_positives": 164,
        "false_positives": 59,
        "false_negatives": 89,
        "true_negatives": 65
      },
      "with_novel": {
        "precision": 0.8430493273542601,
        "recall": 0.6787003610108303,
        "f1_score": 0.7519999999999999,
        "overall_accuracy": 0.7463126843657817,
        "true_positives": 188,
        "true_positives_breakdown": {
          "standard": 164,
          "novel": 24
        },
        "false_positives": 35,
        "false_negatives": 89,
        "true_negatives": 65
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.7876106194690266,
      "total_predictions": 339,
      "correct_predictions": 267,
      "no_misconception_accuracy": 0.7558139534883721,
      "has_misconception_accuracy": 0.7984189723320159,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "openai_o3-mini_effort-medium_single": {
    "provider": "openai_o3-mini_effort-medium",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.6226937269372693,
        "recall": 0.40298507462686567,
        "f1_score": 0.48930772018847407,
        "overall_accuracy": 0.40298507462686567,
        "true_positives": 675,
        "false_positives": 409,
        "false_negatives": 1000,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.8238007380073801,
        "recall": 0.4717379820390914,
        "f1_score": 0.5999328182734296,
        "overall_accuracy": 0.533134328358209,
        "true_positives": 893,
        "true_positives_breakdown": {
          "standard": 675,
          "novel": 218
        },
        "false_positives": 191,
        "false_negatives": 1000,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "openai_o3-mini_effort-medium_multi": {
    "provider": "openai_o3-mini_effort-medium",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.710204081632653,
        "recall": 0.6877470355731226,
        "f1_score": 0.6987951807228915,
        "overall_accuracy": 0.6814159292035398,
        "true_positives": 174,
        "false_positives": 71,
        "false_negatives": 79,
        "true_negatives": 57
      },
      "with_novel": {
        "precision": 0.8244897959183674,
        "recall": 0.7188612099644128,
        "f1_score": 0.7680608365019012,
        "overall_accuracy": 0.7640117994100295,
        "true_positives": 202,
        "true_positives_breakdown": {
          "standard": 174,
          "novel": 28
        },
        "false_positives": 43,
        "false_negatives": 79,
        "true_negatives": 57
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.8053097345132744,
      "total_predictions": 339,
      "correct_predictions": 273,
      "no_misconception_accuracy": 0.6627906976744186,
      "has_misconception_accuracy": 0.8537549407114624,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "anthropic_claude-sonnet-4-5_reasoning_single": {
    "provider": "anthropic_claude-sonnet-4-5_reasoning",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5385139740967961,
        "recall": 0.4716417910447761,
        "f1_score": 0.5028644175684278,
        "overall_accuracy": 0.4716417910447761,
        "true_positives": 790,
        "false_positives": 677,
        "false_negatives": 885,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.6864349011588275,
        "recall": 0.5322410147991543,
        "f1_score": 0.5995832092884786,
        "overall_accuracy": 0.6011940298507462,
        "true_positives": 1007,
        "true_positives_breakdown": {
          "standard": 790,
          "novel": 217
        },
        "false_positives": 460,
        "false_negatives": 885,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "anthropic_claude-sonnet-4-5_reasoning_multi": {
    "provider": "anthropic_claude-sonnet-4-5_reasoning",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.7011070110701108,
        "recall": 0.7509881422924901,
        "f1_score": 0.7251908396946566,
        "overall_accuracy": 0.7109144542772862,
        "true_positives": 190,
        "false_positives": 81,
        "false_negatives": 63,
        "true_negatives": 51
      },
      "with_novel": {
        "precision": 0.8302583025830258,
        "recall": 0.78125,
        "f1_score": 0.8050089445438282,
        "overall_accuracy": 0.8141592920353983,
        "true_positives": 225,
        "true_positives_breakdown": {
          "standard": 190,
          "novel": 35
        },
        "false_positives": 46,
        "false_negatives": 63,
        "true_negatives": 51
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.8466076696165191,
      "total_predictions": 339,
      "correct_predictions": 287,
      "no_misconception_accuracy": 0.5930232558139535,
      "has_misconception_accuracy": 0.932806324110672,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "anthropic_claude-sonnet-4-5_no-reasoning_single": {
    "provider": "anthropic_claude-sonnet-4-5_no-reasoning",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5176701570680629,
        "recall": 0.4722388059701493,
        "f1_score": 0.4939119575398065,
        "overall_accuracy": 0.4722388059701493,
        "true_positives": 791,
        "false_positives": 737,
        "false_negatives": 884,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.6668848167539267,
        "recall": 0.535470310036784,
        "f1_score": 0.5939959195569805,
        "overall_accuracy": 0.6083582089552239,
        "true_positives": 1019,
        "true_positives_breakdown": {
          "standard": 791,
          "novel": 228
        },
        "false_positives": 509,
        "false_negatives": 884,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "anthropic_claude-sonnet-4-5_no-reasoning_multi": {
    "provider": "anthropic_claude-sonnet-4-5_no-reasoning",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.6267123287671232,
        "recall": 0.7233201581027668,
        "f1_score": 0.6715596330275229,
        "overall_accuracy": 0.6460176991150443,
        "true_positives": 183,
        "false_positives": 109,
        "false_negatives": 70,
        "true_negatives": 36
      },
      "with_novel": {
        "precision": 0.7808219178082192,
        "recall": 0.7651006711409396,
        "f1_score": 0.7728813559322034,
        "overall_accuracy": 0.7787610619469026,
        "true_positives": 228,
        "true_positives_breakdown": {
          "standard": 183,
          "novel": 45
        },
        "false_positives": 64,
        "false_negatives": 70,
        "true_negatives": 36
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.8200589970501475,
      "total_predictions": 339,
      "correct_predictions": 278,
      "no_misconception_accuracy": 0.4186046511627907,
      "has_misconception_accuracy": 0.9565217391304348,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "gemini_2.5-flash_reasoning_single": {
    "provider": "gemini_2.5-flash_reasoning",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5366043613707165,
        "recall": 0.41134328358208955,
        "f1_score": 0.46569787090233183,
        "overall_accuracy": 0.41134328358208955,
        "true_positives": 689,
        "false_positives": 595,
        "false_negatives": 986,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.7328660436137072,
        "recall": 0.48832381940840686,
        "f1_score": 0.5861102460292744,
        "overall_accuracy": 0.5617910447761194,
        "true_positives": 941,
        "true_positives_breakdown": {
          "standard": 689,
          "novel": 252
        },
        "false_positives": 343,
        "false_negatives": 986,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "gemini_2.5-flash_reasoning_multi": {
    "provider": "gemini_2.5-flash_reasoning",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.6066666666666667,
        "recall": 0.7193675889328063,
        "f1_score": 0.6582278481012658,
        "overall_accuracy": 0.6253687315634219,
        "true_positives": 182,
        "false_positives": 118,
        "false_negatives": 71,
        "true_negatives": 30
      },
      "with_novel": {
        "precision": 0.76,
        "recall": 0.7625418060200669,
        "f1_score": 0.7612687813021702,
        "overall_accuracy": 0.7610619469026548,
        "true_positives": 228,
        "true_positives_breakdown": {
          "standard": 182,
          "novel": 46
        },
        "false_positives": 72,
        "false_negatives": 71,
        "true_negatives": 30
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.8082595870206489,
      "total_predictions": 339,
      "correct_predictions": 274,
      "no_misconception_accuracy": 0.3488372093023256,
      "has_misconception_accuracy": 0.9644268774703557,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "gemini_2.5-flash_no-reasoning_single": {
    "provider": "gemini_2.5-flash_no-reasoning",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5805369127516778,
        "recall": 0.41313432835820896,
        "f1_score": 0.4827345657481688,
        "overall_accuracy": 0.41313432835820896,
        "true_positives": 692,
        "false_positives": 500,
        "false_negatives": 983,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.75,
        "recall": 0.47629195524773577,
        "f1_score": 0.5826001955034212,
        "overall_accuracy": 0.5337313432835821,
        "true_positives": 894,
        "true_positives_breakdown": {
          "standard": 692,
          "novel": 202
        },
        "false_positives": 298,
        "false_negatives": 983,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "gemini_2.5-flash_no-reasoning_multi": {
    "provider": "gemini_2.5-flash_no-reasoning",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.7098445595854922,
        "recall": 0.541501976284585,
        "f1_score": 0.6143497757847535,
        "overall_accuracy": 0.56047197640118,
        "true_positives": 137,
        "false_positives": 56,
        "false_negatives": 116,
        "true_negatives": 53
      },
      "with_novel": {
        "precision": 0.7875647668393783,
        "recall": 0.5671641791044776,
        "f1_score": 0.6594360086767895,
        "overall_accuracy": 0.6047197640117994,
        "true_positives": 152,
        "true_positives_breakdown": {
          "standard": 137,
          "novel": 15
        },
        "false_positives": 41,
        "false_negatives": 116,
        "true_negatives": 53
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.6283185840707964,
      "total_predictions": 339,
      "correct_predictions": 213,
      "no_misconception_accuracy": 0.6162790697674418,
      "has_misconception_accuracy": 0.6324110671936759,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "vllm_qwen3-8b_thinking_single": {
    "provider": "vllm_qwen3-8b_thinking",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5221445221445221,
        "recall": 0.26746268656716415,
        "f1_score": 0.353730754046585,
        "overall_accuracy": 0.26746268656716415,
        "true_positives": 448,
        "false_positives": 410,
        "false_negatives": 1227,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.8263403263403264,
        "recall": 0.3662190082644628,
        "f1_score": 0.5075161059413028,
        "overall_accuracy": 0.42328358208955225,
        "true_positives": 709,
        "true_positives_breakdown": {
          "standard": 448,
          "novel": 261
        },
        "false_positives": 149,
        "false_negatives": 1227,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "vllm_qwen3-8b_thinking_multi": {
    "provider": "vllm_qwen3-8b_thinking",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.565,
        "recall": 0.44664031620553357,
        "f1_score": 0.49889624724061804,
        "overall_accuracy": 0.4778761061946903,
        "true_positives": 113,
        "false_positives": 87,
        "false_negatives": 140,
        "true_negatives": 49
      },
      "with_novel": {
        "precision": 0.73,
        "recall": 0.5104895104895105,
        "f1_score": 0.6008230452674898,
        "overall_accuracy": 0.5752212389380531,
        "true_positives": 146,
        "true_positives_breakdown": {
          "standard": 113,
          "novel": 33
        },
        "false_positives": 54,
        "false_negatives": 140,
        "true_negatives": 49
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.6253687315634219,
      "total_predictions": 339,
      "correct_predictions": 212,
      "no_misconception_accuracy": 0.5697674418604651,
      "has_misconception_accuracy": 0.6442687747035574,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "vllm_qwen3-8b_no-thinking_single": {
    "provider": "vllm_qwen3-8b_no-thinking",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.4802547770700637,
        "recall": 0.22507462686567165,
        "f1_score": 0.3065040650406504,
        "overall_accuracy": 0.22507462686567165,
        "true_positives": 377,
        "false_positives": 408,
        "false_negatives": 1298,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.8229299363057325,
        "recall": 0.3323045267489712,
        "f1_score": 0.47343349212165625,
        "overall_accuracy": 0.3856716417910448,
        "true_positives": 646,
        "true_positives_breakdown": {
          "standard": 377,
          "novel": 269
        },
        "false_positives": 139,
        "false_negatives": 1298,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "vllm_qwen3-8b_no-thinking_multi": {
    "provider": "vllm_qwen3-8b_no-thinking",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.5617283950617284,
        "recall": 0.35968379446640314,
        "f1_score": 0.4385542168674699,
        "overall_accuracy": 0.4424778761061947,
        "true_positives": 91,
        "false_positives": 71,
        "false_negatives": 162,
        "true_negatives": 59
      },
      "with_novel": {
        "precision": 0.7592592592592593,
        "recall": 0.43157894736842106,
        "f1_score": 0.5503355704697986,
        "overall_accuracy": 0.5368731563421829,
        "true_positives": 123,
        "true_positives_breakdown": {
          "standard": 91,
          "novel": 32
        },
        "false_positives": 39,
        "false_negatives": 162,
        "true_negatives": 59
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.5722713864306784,
      "total_predictions": 339,
      "correct_predictions": 194,
      "no_misconception_accuracy": 0.686046511627907,
      "has_misconception_accuracy": 0.5335968379446641,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "vllm_qwen3-14b_thinking_single": {
    "provider": "vllm_qwen3-14b_thinking",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.4721115537848606,
        "recall": 0.2829850746268657,
        "f1_score": 0.35386338185890265,
        "overall_accuracy": 0.2829850746268657,
        "true_positives": 474,
        "false_positives": 530,
        "false_negatives": 1201,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.7838645418326693,
        "recall": 0.39587525150905434,
        "f1_score": 0.5260695187165776,
        "overall_accuracy": 0.4698507462686567,
        "true_positives": 787,
        "true_positives_breakdown": {
          "standard": 474,
          "novel": 313
        },
        "false_positives": 217,
        "false_negatives": 1201,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "vllm_qwen3-14b_thinking_multi": {
    "provider": "vllm_qwen3-14b_thinking",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.5826446280991735,
        "recall": 0.5573122529644269,
        "f1_score": 0.5696969696969696,
        "overall_accuracy": 0.5073746312684366,
        "true_positives": 141,
        "false_positives": 101,
        "false_negatives": 112,
        "true_negatives": 31
      },
      "with_novel": {
        "precision": 0.7024793388429752,
        "recall": 0.6028368794326241,
        "f1_score": 0.648854961832061,
        "overall_accuracy": 0.5929203539823009,
        "true_positives": 170,
        "true_positives_breakdown": {
          "standard": 141,
          "novel": 29
        },
        "false_positives": 72,
        "false_negatives": 112,
        "true_negatives": 31
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.6430678466076696,
      "total_predictions": 339,
      "correct_predictions": 218,
      "no_misconception_accuracy": 0.36046511627906974,
      "has_misconception_accuracy": 0.7391304347826086,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "vllm_qwen3-14b_no-thinking_single": {
    "provider": "vllm_qwen3-14b_no-thinking",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.4814004376367615,
        "recall": 0.2626865671641791,
        "f1_score": 0.33989957512553104,
        "overall_accuracy": 0.2626865671641791,
        "true_positives": 440,
        "false_positives": 474,
        "false_negatives": 1235,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.8030634573304157,
        "recall": 0.3727780599288979,
        "f1_score": 0.5091918140825528,
        "overall_accuracy": 0.4382089552238806,
        "true_positives": 734,
        "true_positives_breakdown": {
          "standard": 440,
          "novel": 294
        },
        "false_positives": 180,
        "false_negatives": 1235,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "vllm_qwen3-14b_no-thinking_multi": {
    "provider": "vllm_qwen3-14b_no-thinking",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.6350710900473934,
        "recall": 0.5296442687747036,
        "f1_score": 0.5775862068965517,
        "overall_accuracy": 0.5781710914454278,
        "true_positives": 134,
        "false_positives": 77,
        "false_negatives": 119,
        "true_negatives": 62
      },
      "with_novel": {
        "precision": 0.8151658767772512,
        "recall": 0.5910652920962199,
        "f1_score": 0.6852589641434262,
        "overall_accuracy": 0.6902654867256637,
        "true_positives": 172,
        "true_positives_breakdown": {
          "standard": 134,
          "novel": 38
        },
        "false_positives": 39,
        "false_negatives": 119,
        "true_negatives": 62
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.7345132743362832,
      "total_predictions": 339,
      "correct_predictions": 249,
      "no_misconception_accuracy": 0.7209302325581395,
      "has_misconception_accuracy": 0.7391304347826086,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  }
}