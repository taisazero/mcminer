{
  "openai_o3-mini_effort-low_single": {
    "provider": "openai_o3-mini_effort-low",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.6358436606291706,
        "recall": 0.3982089552238806,
        "f1_score": 0.4897209985315712,
        "overall_accuracy": 0.3982089552238806,
        "true_positives": 667,
        "false_positives": 382,
        "false_negatives": 1008,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.8312678741658722,
        "recall": 0.46382978723404256,
        "f1_score": 0.595425059747354,
        "overall_accuracy": 0.5205970149253731,
        "true_positives": 872,
        "true_positives_breakdown": {
          "standard": 667,
          "novel": 205
        },
        "false_positives": 177,
        "false_negatives": 1008,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "openai_o3-mini_effort-low_multi": {
    "provider": "openai_o3-mini_effort-low",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.7219730941704036,
        "recall": 0.6363636363636364,
        "f1_score": 0.676470588235294,
        "overall_accuracy": 0.6666666666666666,
        "true_positives": 161,
        "false_positives": 62,
        "false_negatives": 92,
        "true_negatives": 65
      },
      "with_novel": {
        "precision": 0.8565022421524664,
        "recall": 0.6749116607773852,
        "f1_score": 0.7549407114624508,
        "overall_accuracy": 0.7551622418879056,
        "true_positives": 191,
        "true_positives_breakdown": {
          "standard": 161,
          "novel": 30
        },
        "false_positives": 32,
        "false_negatives": 92,
        "true_negatives": 65
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.7876106194690266,
      "total_predictions": 339,
      "correct_predictions": 267,
      "no_misconception_accuracy": 0.7558139534883721,
      "has_misconception_accuracy": 0.7984189723320159,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "openai_o3-mini_effort-medium_single": {
    "provider": "openai_o3-mini_effort-medium",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.6392988929889298,
        "recall": 0.4137313432835821,
        "f1_score": 0.5023559260601667,
        "overall_accuracy": 0.4137313432835821,
        "true_positives": 693,
        "false_positives": 391,
        "false_negatives": 982,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.8293357933579336,
        "recall": 0.4779372674109516,
        "f1_score": 0.6064080944350758,
        "overall_accuracy": 0.5367164179104478,
        "true_positives": 899,
        "true_positives_breakdown": {
          "standard": 693,
          "novel": 206
        },
        "false_positives": 185,
        "false_negatives": 982,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "openai_o3-mini_effort-medium_multi": {
    "provider": "openai_o3-mini_effort-medium",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.689795918367347,
        "recall": 0.6679841897233202,
        "f1_score": 0.6787148594377511,
        "overall_accuracy": 0.6666666666666666,
        "true_positives": 169,
        "false_positives": 76,
        "false_negatives": 84,
        "true_negatives": 57
      },
      "with_novel": {
        "precision": 0.8326530612244898,
        "recall": 0.7083333333333334,
        "f1_score": 0.7654784240150095,
        "overall_accuracy": 0.7699115044247787,
        "true_positives": 204,
        "true_positives_breakdown": {
          "standard": 169,
          "novel": 35
        },
        "false_positives": 41,
        "false_negatives": 84,
        "true_negatives": 57
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.8053097345132744,
      "total_predictions": 339,
      "correct_predictions": 273,
      "no_misconception_accuracy": 0.6627906976744186,
      "has_misconception_accuracy": 0.8537549407114624,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "anthropic_claude-sonnet-4-5_reasoning_single": {
    "provider": "anthropic_claude-sonnet-4-5_reasoning",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5289706884798909,
        "recall": 0.4632835820895522,
        "f1_score": 0.4939528962444303,
        "overall_accuracy": 0.4632835820895522,
        "true_positives": 776,
        "false_positives": 691,
        "false_negatives": 899,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.6912065439672802,
        "recall": 0.5300575013068479,
        "f1_score": 0.6000000000000001,
        "overall_accuracy": 0.6053731343283583,
        "true_positives": 1014,
        "true_positives_breakdown": {
          "standard": 776,
          "novel": 238
        },
        "false_positives": 453,
        "false_negatives": 899,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "anthropic_claude-sonnet-4-5_reasoning_multi": {
    "provider": "anthropic_claude-sonnet-4-5_reasoning",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.6863468634686347,
        "recall": 0.7351778656126482,
        "f1_score": 0.7099236641221375,
        "overall_accuracy": 0.6991150442477876,
        "true_positives": 186,
        "false_positives": 85,
        "false_negatives": 67,
        "true_negatives": 51
      },
      "with_novel": {
        "precision": 0.8376383763837638,
        "recall": 0.7721088435374149,
        "f1_score": 0.8035398230088496,
        "overall_accuracy": 0.8200589970501475,
        "true_positives": 227,
        "true_positives_breakdown": {
          "standard": 186,
          "novel": 41
        },
        "false_positives": 44,
        "false_negatives": 67,
        "true_negatives": 51
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.8466076696165191,
      "total_predictions": 339,
      "correct_predictions": 287,
      "no_misconception_accuracy": 0.5930232558139535,
      "has_misconception_accuracy": 0.932806324110672,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "gemini_2.5-flash_reasoning_single": {
    "provider": "gemini_2.5-flash_reasoning",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5482866043613707,
        "recall": 0.42029850746268654,
        "f1_score": 0.4758364312267658,
        "overall_accuracy": 0.42029850746268654,
        "true_positives": 704,
        "false_positives": 580,
        "false_negatives": 971,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.7367601246105919,
        "recall": 0.4934793948878456,
        "f1_score": 0.5910652920962199,
        "overall_accuracy": 0.5647761194029851,
        "true_positives": 946,
        "true_positives_breakdown": {
          "standard": 704,
          "novel": 242
        },
        "false_positives": 338,
        "false_negatives": 971,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "gemini_2.5-flash_reasoning_multi": {
    "provider": "gemini_2.5-flash_reasoning",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.61,
        "recall": 0.7233201581027668,
        "f1_score": 0.6618444846292947,
        "overall_accuracy": 0.6283185840707964,
        "true_positives": 183,
        "false_positives": 117,
        "false_negatives": 70,
        "true_negatives": 30
      },
      "with_novel": {
        "precision": 0.77,
        "recall": 0.7674418604651163,
        "f1_score": 0.7687188019966723,
        "overall_accuracy": 0.7699115044247787,
        "true_positives": 231,
        "true_positives_breakdown": {
          "standard": 183,
          "novel": 48
        },
        "false_positives": 69,
        "false_negatives": 70,
        "true_negatives": 30
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.8082595870206489,
      "total_predictions": 339,
      "correct_predictions": 274,
      "no_misconception_accuracy": 0.3488372093023256,
      "has_misconception_accuracy": 0.9644268774703557,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "anthropic_claude-sonnet-4-5_no-reasoning_single": {
    "provider": "anthropic_claude-sonnet-4-5_no-reasoning",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5157068062827225,
        "recall": 0.47044776119402987,
        "f1_score": 0.49203871370590074,
        "overall_accuracy": 0.47044776119402987,
        "true_positives": 788,
        "false_positives": 740,
        "false_negatives": 887,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.6747382198952879,
        "recall": 0.5375391032325338,
        "f1_score": 0.5983749274521184,
        "overall_accuracy": 0.6155223880597015,
        "true_positives": 1031,
        "true_positives_breakdown": {
          "standard": 788,
          "novel": 243
        },
        "false_positives": 497,
        "false_negatives": 887,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "anthropic_claude-sonnet-4-5_no-reasoning_multi": {
    "provider": "anthropic_claude-sonnet-4-5_no-reasoning",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.6404109589041096,
        "recall": 0.7391304347826086,
        "f1_score": 0.6862385321100918,
        "overall_accuracy": 0.6578171091445427,
        "true_positives": 187,
        "false_positives": 105,
        "false_negatives": 66,
        "true_negatives": 36
      },
      "with_novel": {
        "precision": 0.791095890410959,
        "recall": 0.7777777777777778,
        "f1_score": 0.7843803056027165,
        "overall_accuracy": 0.7876106194690266,
        "true_positives": 231,
        "true_positives_breakdown": {
          "standard": 187,
          "novel": 44
        },
        "false_positives": 61,
        "false_negatives": 66,
        "true_negatives": 36
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.8200589970501475,
      "total_predictions": 339,
      "correct_predictions": 278,
      "no_misconception_accuracy": 0.4186046511627907,
      "has_misconception_accuracy": 0.9565217391304348,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  },
  "gemini_2.5-flash_no-reasoning_single": {
    "provider": "gemini_2.5-flash_no-reasoning",
    "mode": "single",
    "claude_eval": {
      "standard": {
        "precision": 0.5906040268456376,
        "recall": 0.42029850746268654,
        "f1_score": 0.49110568538542027,
        "overall_accuracy": 0.42029850746268654,
        "true_positives": 704,
        "false_positives": 488,
        "false_negatives": 971,
        "true_negatives": 0
      },
      "with_novel": {
        "precision": 0.7550335570469798,
        "recall": 0.48102618920363444,
        "f1_score": 0.5876591576885407,
        "overall_accuracy": 0.5373134328358209,
        "true_positives": 900,
        "true_positives_breakdown": {
          "standard": 704,
          "novel": 196
        },
        "false_positives": 292,
        "false_negatives": 971,
        "true_negatives": 0
      },
      "novel_validation_enabled": true
    },
    "gt_based": {},
    "total_predictions": 1675
  },
  "gemini_2.5-flash_no-reasoning_multi": {
    "provider": "gemini_2.5-flash_no-reasoning",
    "mode": "multi",
    "claude_eval": {
      "standard": {
        "precision": 0.689119170984456,
        "recall": 0.525691699604743,
        "f1_score": 0.5964125560538117,
        "overall_accuracy": 0.5486725663716814,
        "true_positives": 133,
        "false_positives": 60,
        "false_negatives": 120,
        "true_negatives": 53
      },
      "with_novel": {
        "precision": 0.7979274611398963,
        "recall": 0.5620437956204379,
        "f1_score": 0.6595289079229122,
        "overall_accuracy": 0.6106194690265486,
        "true_positives": 154,
        "true_positives_breakdown": {
          "standard": 133,
          "novel": 21
        },
        "false_positives": 39,
        "false_negatives": 120,
        "true_negatives": 53
      },
      "novel_validation_enabled": true
    },
    "gt_based": {
      "overall_accuracy": 0.6283185840707964,
      "total_predictions": 339,
      "correct_predictions": 213,
      "no_misconception_accuracy": 0.6162790697674418,
      "has_misconception_accuracy": 0.6324110671936759,
      "no_misconception_samples": 86,
      "has_misconception_samples": 253
    },
    "total_predictions": 339
  }
}